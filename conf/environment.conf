#!/bin/sh

##############################################
###    User Defined Cluster TOP Dir        ###
##############################################
export SPARK_HOME=
export EGO_TOP=
export HADOOP_HOME=
export SYM_MASTER_HOST=
export TEST_TOOL_HOME=
export JAVA_HOME=

##############################################
###    Cluster Characters for Case Filter  ###
##############################################
export SLOTS_PER_HOST=6     #Slots of each compute host, specify for schedule and reclaim
export HOST_NUM=1            #Host number of ego cluster, specify for HA. default: 1
export DIST_FILE_SYSTEM=HDFS #Distrubuted file System in use, "GPFS" or "HDFS". default: HDFS

##############################################
###    Env for Test, change if needed      ###
##############################################
export SUBMIT_USER=root      #OS User to submit spark workload
export SAMPLE_JAR=$TEST_TOOL_HOME/autoTestExamples.jar #out-of-box samples for test tool
export MASTER_LOG=$SPARK_HOME/logs/spark-$SUBMIT_USER-org.apache.spark.deploy.master.Master-1-$SYM_MASTER_HOST.out
export CASE_RUNNING_TIMEOUT=600    #each case will be killed if it keeps running beyond the timeout, in second
export SPARK_CONF_DIR=$SPARK_HOME/conf 
