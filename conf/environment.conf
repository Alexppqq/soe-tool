#!/bin/sh

##############################################
###    User Defined Cluster TOP Dir        ###
##############################################
export SPARK_HOME=/gpfs/gaop/spark161/spark/spark-1.6.1-hadoop-2.6/
export HADOOP_HOME=
export EGO_TOP=/gpfs/gaop/conductor-1.1
export SYM_MASTER_HOST=red03
export TEST_TOOL_HOME=/usr/local/gaop/running
export JAVA_HOME=/gpfs/gaop/conductor-1.1/jre/3.3/linux-x86_64/
##############################################
###    Runtime environment:conductor/core  ###
##############################################
export RUNTIME_ENV=conductor  
#Total free lots of each compute host, specify for schedule and reclaim
export SLOTS_PER_HOST=16
#Host number of ego cluster, specify for HA
export HOST_NUM=1
##############################################
###    Cluster Characters for Case Filter  ###
##############################################
#Distrubuted file System in use, "GPFS" or "HDFS". default: HDFS
export DIST_FILE_SYSTEM=GPFS

##############################################
###    Env for Test, change if needed      ###
##############################################
export SUBMIT_USER=root      #OS User to submit spark workload
export EXECUTION_USER=root
export SAMPLE_JAR=$TEST_TOOL_HOME/lib/autoTestExamples.jar #out-of-box samples for test tool
export MASTER_LOG=$SPARK_HOME/logs/spark-$SUBMIT_USER-org.apache.spark.deploy.master.Master-1-$SYM_MASTER_HOST.out
export CASE_RUNNING_TIMEOUT=600    #each case will be killed if it keeps running beyond the timeout, in second
export SPARK_CONF_DIR=$SPARK_HOME/conf 
