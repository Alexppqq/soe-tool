#!/bin/sh

##############################################
###    User Defined Cluster TOP Dir        ###
##############################################
export SPARK_HOME=/opt/gaop/spark-1.6.0-bin-hadoop2.6/
export HADOOP_HOME=/opt/hadoop-2.7.1
#export EGO_TOP=/opt/gaop/sym_ego3.1/
export EGO_TOP=/opt/gaop/sym_ego3.1/profile.platform
export SYM_MASTER_HOST=red01
export TEST_TOOL_HOME=/usr/local/gaop/running
export JAVA_HOME=/pcc/lsfqa-trusted/3rdparty/jdk/ibm_8.0-1.0/linux-x86_64

#Total free lots of each compute host, specify for schedule and reclaim
export SLOTS_PER_HOST=16
#Host number of ego cluster, specify for HA
export HOST_NUM=1
##############################################
###    Cluster Characters for Case Filter  ###
##############################################
#Distrubuted file System in use, "GPFS" or "HDFS". default: HDFS
export DIST_FILE_SYSTEM=HDFS 

##############################################
###    Env for Test, change if needed      ###
##############################################
export SUBMIT_USER=root      #OS User to submit spark workload
export SAMPLE_JAR=$TEST_TOOL_HOME/lib/autoTestExamples.jar #out-of-box samples for test tool
export MASTER_LOG=$SPARK_HOME/logs/spark-$SUBMIT_USER-org.apache.spark.deploy.master.Master-1-$SYM_MASTER_HOST.out
export CASE_RUNNING_TIMEOUT=600    #each case will be killed if it keeps running beyond the timeout, in second
export SPARK_CONF_DIR=$SPARK_HOME/conf 
