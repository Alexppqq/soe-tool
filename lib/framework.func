#!/bin/sh

###############################
###   function of framework
###############################
function fw_create_report_dir ()
{
  global_report_dir=$TEST_TOOL_HOME/reports/report-`date +%s`
  mkdir -p $global_report_dir
  mkdir -p $global_report_dir/logs
  global_test_report=$global_report_dir/testReport.txt
}
function sc_recover_spark_conf ()
{
   #all files as a atomic operation
   if [[ -f $SPARK_CONF_DIR/spark-env.sh.org.bak && -f $SPARK_CONF_DIR/spark-defaults.conf.org.bak && \
                -f $SPARK_CONF_DIR/log4j.properties.org.bak  ]]; then
      mv -f $SPARK_CONF_DIR/spark-env.sh.org.bak $SPARK_CONF_DIR/spark-env.sh
      mv -f $SPARK_CONF_DIR/spark-defaults.conf.org.bak $SPARK_CONF_DIR/spark-defaults.conf
      mv -f $SPARK_CONF_DIR/log4j.properties.org.bak $SPARK_CONF_DIR/log4j.properties
   else
      echo "error: sc_recover_spark_conf: cannot recover spark conf."
      exit 1;
   fi
}
function ca_recover_and_exit ()
{
   sc_recover_spark_conf
   if [ -n $1 ]; then
      exit $1
   else
      exit 0
   fi
}
function fw_report_write_title ()
{
  global_test_start_time=`date`
  echo "---------------------------------------------------------------------"   &>> $global_test_report
  echo ""  &>> $global_test_report
  echo "                      Regression Test Report       "   &>> $global_test_report
  echo ""  &>> $global_test_report
  echo "---------------------------------------------------------------------"   &>> $global_test_report
  echo -n "     Ego Version: " &>> $global_test_report; egosh -V 1>/dev/null 2>> $global_test_report
  echo  "" >> $global_test_report
  echo -e "     SparkOnEgo Version: "`$SPARK_HOME/bin/spark-submit --egoversion`  &>> $global_test_report
  echo -e "     Date: " $global_test_start_time  &>> $global_test_report
  echo "---------------------------------------------------------------------"   &>> $global_test_report
}
function fw_report_save_case_result_in_file ()
{
  if [[ "$#" -eq 3 && -n "$1" && -n "$2" && -n "$3" ]]; then 
     echo -e "${1}\n${2}\n${3}" > $global_case_log_dir/caseResult
  elif [[ "$#" -eq 2 && -n "$1" && -n "$2" ]]; then
     echo -e "${1}\n${2}" > $global_case_log_dir/caseResult
  else 
     echo "error: ./fw_report_save_case_result_in_file: parameters should be CaseName CaseResult Reason."
     ca_recover_and_exit 1;
  fi
}
function fw_report_write_case_result_to_report ()
{
  if [[ -f $global_case_log_dir/caseResult ]]; then
     cat $global_case_log_dir/caseResult | while read line; do
         echo -e "$line \c" >> $global_test_report
         echo -e "$line \c" >> $global_report_dir/caseResultList
     done
     echo  "" >> $global_test_report #newline
     echo  "" >> $global_report_dir/caseResultList
     rm -rf $global_case_log_dir/caseResult
  else
     echo "error: file of case result ($global_case_log_dir/caseResult) cannot be found."
  fi
}
function fw_report_calculate_statis ()
{
  global_test_end_time=`date`
  #could rewrite it with grep -c
  global_case_pass=`sed -n '/Pass/'p $global_report_dir/caseResultList | wc -l`
  global_case_fail=`sed -n '/Fail/'p $global_report_dir/caseResultList | wc -l`
  global_case_skip=`sed -n '/Skip/'p $global_report_dir/caseResultList | wc -l`
  global_case_timeout=`sed -n '/Timeout/'p $global_report_dir/caseResultList | wc -l`
  global_case_total=`expr $global_case_pass + $global_case_fail + $global_case_timeout`
  global_case_pass_rate=0
  if [[ "$global_case_total" -gt "0" ]]; then
     global_case_pass_rate=`awk 'BEGIN{printf "%.2f%%",('$global_case_pass' * 100 / '$global_case_total')}'`   
  fi
  echo "----------------------------------------------------------------------"   &>> $global_test_report
  echo ""   &>> $global_test_report
  echo "     Total: $global_case_total" &>> $global_test_report
  echo "     Pass: $global_case_pass" &>> $global_test_report
  echo "     Fail: $global_case_fail" &>> $global_test_report
  echo "     Timeout: $global_case_timeout" &>> $global_test_report
  echo "     Skip: $global_case_skip" &>>  $global_test_report
  echo "     Pass Rate(Pass/<Fail+Timeout>): $global_case_pass_rate" &>> $global_test_report
  echo ""   &>> $global_test_report
  echo "----------------------------------------------------------------------"   &>> $global_test_report
}
function sc_backup_spark_conf ()
{
   #all files as a atomic operation
   if [[ -f $SPARK_CONF_DIR/spark-env.sh && -f $SPARK_CONF_DIR/spark-defaults.conf && \
                -f $SPARK_CONF_DIR/log4j.properties  ]]; then
      cp -f $SPARK_CONF_DIR/spark-env.sh $SPARK_CONF_DIR/spark-env.sh.org.bak
      cp -f $SPARK_CONF_DIR/spark-defaults.conf $SPARK_CONF_DIR/spark-defaults.conf.org.bak
      cp -f $SPARK_CONF_DIR/log4j.properties $SPARK_CONF_DIR/log4j.properties.org.bak
   else  
      echo "error: sc_backup_spark_conf: cannot backup spark conf:spark-env.sh spark-defaults.conf log4j.properties" 
      exit 1
   fi
}
function fw_get_file_type ()
{
  if [[ -z "$1" || -n $2 ]]; then
     echo "error: no file specified or too many specified."
  elif [[ ! -f $1 ]]; then
     echo "error: $1 is not a valid file name."
  fi
  echo $1|awk -F "." '{print $NF}'
}
## find .sh file with executable right and .bats file recursively
function fw_print_case ()
{
   ### validation check
   if [[ -z "$1" || -n $2 ]]; then
      echo "error: ./fw_print_case: no dir/file specified or too many parameters specified."
      exit 1
   fi

   if [[ -f $1 ]]; then
      local fileType=`fw_get_file_type $1`
      if [[ "$fileType" == "sh" && -x $1 ]]; then
         echo $1
      elif [[ "$fileType" == "bats" ]]; then
         echo $1
      fi
   elif [[ -d $1 && -x $1 ]]; then
      for item in $1/*; do
              fw_print_case $item
      done
   fi
}
function fw_create_case_list ()
{
   fw_print_case $1 >>  $global_report_dir/caseList
}

## configuration
function sc_update_to_spark_env ()
{
   if [[ "$#" -ne 2 || -z "$1" || -z "$2" ]]; then
      echo "usage: ./sc_update_to_spark_env envName envValue"
      echo "please make sure use it after backup customer's conf"
      ca_recover_and_exit 1;
   elif [[ ! -f $SPARK_CONF_DIR/spark-env.sh ]]; then
      echo "error: ./sc_update_to_spark_env: please make sure $SPARK_CONF_DIR/spark-env.sh is exist."
      ca_recover_and_exit 1;
   fi
   local envName=$1
   local envValue=$2
   local OLD_SETTING_DEF=`cat $SPARK_CONF_DIR/spark-env.sh | grep "^$envName="`
   if [ -z "$OLD_SETTING_DEF" ]; then
       local lines=`cat $SPARK_CONF_DIR/spark-env.sh|wc -l`
       if [ $lines -ne 0 ]; then
           sed -i --follow-symlinks "\$a$envName=$envValue" $SPARK_CONF_DIR/spark-env.sh
       else
           echo "$envName=$envValue" >> $SPARK_CONF_DIR/spark-env.sh
       fi
   else
       sed -i --follow-symlinks -e "s#^$envName=#\#$envName=#g" $SPARK_CONF_DIR/spark-env.sh
       sed -i --follow-symlinks "\$a$envName=$envValue" $SPARK_CONF_DIR/spark-env.sh
   fi
}

function sc_update_to_spark_default ()
{
   if [[ "$#" -ne 2 || -z "$1" || -z "$2" ]]; then
      echo "usage: ./sc_update_to_spark_default  envName envValue"
      echo "please make sure use it after backup customer's conf"
      ca_recover_and_exit 1;
   elif [[ ! -f $SPARK_CONF_DIR/spark-defaults.conf ]]; then
      echo "error: ./sc_update_to_spark_default: please make sure $SPARK_CONF_DIR/spark-defaults.conf is exist."
      ca_recover_and_exit 1;
   fi
   local envName=$1
   local envValue=$2
   #below grep cmd must have a spcace in the end
   local OLD_SETTING_DEF=$(grep "^$envName" $SPARK_CONF_DIR/spark-defaults.conf || echo "")
   if [ -z "$OLD_SETTING_DEF" ]; then
       local lines=`cat $SPARK_CONF_DIR/spark-defaults.conf|wc -l`
       if [ $lines -ne 0 ]; then
           sed -i --follow-symlinks "\$a$envName $envValue" $SPARK_CONF_DIR/spark-defaults.conf
       else
          # "$lines -eq 0"
          echo "$envName $envValue" >> $SPARK_CONF_DIR/spark-defaults.conf
       fi
   else
       sed -i --follow-symlinks -e "s#^$envName=#\#$envName=#g" $SPARK_CONF_DIR/spark-defaults.conf
       sed -i --follow-symlinks -e "s#^$envName #\#$envName #g" $SPARK_CONF_DIR/spark-defaults.conf
       sed -i --follow-symlinks "\$a$envName $envValue" $SPARK_CONF_DIR/spark-defaults.conf
   fi
}

function sc_comment_out_in_spark_env ()
{
   if [[ "$#" -ne 1 || -z "$1" ]]; then
      echo "usage: ./sc_comment_out_in_spark_env envName"
      echo "please make sure use it after backup customer's conf"
      ca_recover_and_exit 1;
   fi
   local envName=$1
   local OLD_SETTING_DEF=`cat $SPARK_CONF_DIR/spark-env.sh | grep "^$envName="`
   if [ -n "$OLD_SETTING_DEF" ]; then
       sed -i --follow-symlinks -e "s#^$envName=#\#$envName=#g" $SPARK_CONF_DIR/spark-env.sh
   fi
}

function sc_comment_out_in_spark_default ()
{
  if [[ "$#" -ne 1 || -z "$1" ]]; then
      echo "usage: ./sc_comment_out_in_spark_default envName"
      echo "please make sure use it after backup customer's conf"
      ca_recover_and_exit 1;
  fi
  local envName=$1
  local OLD_SETTING_DEF=$(grep "^$envName" $SPARK_CONF_DIR/spark-defaults.conf || echo "")
  if [ -n "$OLD_SETTING_DEF" ]; then
       sed -i --follow-symlinks -e "s#^$envName=#\#$envName=#g" $SPARK_CONF_DIR/spark-defaults.conf
       sed -i --follow-symlinks -e "s#^$envName #\#$envName #g" $SPARK_CONF_DIR/spark-defaults.conf
  fi
}

function ca_update_to_spark_log4j ()
{
   if [[ "$#" -ne 2 || -z "$1" || -z "$2" ]]; then
      echo "usage: ./ca_update_to_spark_log4j envName envValue"
      echo "please make sure use it after backup customer's conf"
      ca_recover_and_exit 1;
   elif [[ ! -f $SPARK_CONF_DIR/log4j.properties ]]; then
      echo "error: ./ca_update_to_spark_log4j: please make sure $SPARK_CONF_DIR/log4j.properties is exist."
      ca_recover_and_exit 1;
   fi
   local envName=$1
   local envValue=$2
   local OLD_SETTING_DEF=`cat $SPARK_CONF_DIR/log4j.properties | grep "^$envName="`
   if [ -z "$OLD_SETTING_DEF" ]; then
       local lines=`cat $SPARK_CONF_DIR/log4j.properties|wc -l`
       if [ $lines -ne 0 ]; then
          sed -i --follow-symlinks "\$a$envName=$envValue" $SPARK_CONF_DIR/log4j.properties
       else
          echo "$envName=$envValue" >> $SPARK_CONF_DIR/log4j.properties
       fi
   else
       sed -i --follow-symlinks -e "s#^$envName=#\#$envName=#g" $SPARK_CONF_DIR/log4j.properties
       echo "$envName=$envValue" >> $SPARK_CONF_DIR/log4j.properties
   fi
}

function ca_enable_shuffle_service ()
{
   sc_update_to_spark_default "spark.shuffle.service.enabled" "true"
}

function sc_open_debug_log_4tag ()
{
   ca_update_to_spark_log4j "org.apache.spark.util.EGOSparkJsonConfig" "DEBUG"
   ca_update_to_spark_log4j "log4j.logger.org.apache.spark.deploy.master.policy.hierarchy.PolicyHierarchy" "DEBUG"
   ca_update_to_spark_log4j "log4j.logger.org.apache.spark.deploy.master.EGOResourceManager" "DEBUG"
   ca_update_to_spark_log4j "log4j.logger.org.apache.spark.deploy.master.Master" "DEBUG"
}

function sc_config_policy ()
{
   if [[ "$#" -ne 1 || -z "$1" ]]; then
      echo "error: ./sc_config_policy: no policy specified."
      ca_recover_and_exit 1
   else
      sc_update_to_spark_default "spark.ego.app.schedule.policy" "$1"
   fi
}

function sc_restart_master_by_ego_service ()
{
   local currEgosh=`which egosh|grep "egosh$"`
   if [[ -z "$currEgosh" ]]; then
      echo "error: ./sc_restart_master_by_ego_service: no egosh be found."
      ca_recover_and_exit 1;
   fi
   egosh service stop SPARKMaster; sleep 3; egosh service start SPARKMaster
}

function sc_restart_master_by_script ()
{
   $SPARK_HOME/sbin/stop-master.sh; sleep 3; $SPARK_HOME/sbin/start-master.sh
}

function ca_get_policy_inuse ()
{
   Policy_In_Use=`grep -m1 "Master application schedule policy" $MASTER_LOG| awk -F " " '{print $NF}'`
   export Policy_In_Use;
}

function sc_verify_policy_take_effect ()
{
   if [[ "$#" -ne 1 || -z "$1" ]]; then
      echo "error: sc_verify_policy_take_effect: please specify expectd policy."
      ca_recover_and_exit 1;
   fi
   local policyInUse=`grep -m1 "Master application schedule policy" $MASTER_LOG| awk -F " " '{print $NF}'`
   #echo "current policy is $policyInUse."
   if [[ "$policyInUse" != "$1" ]]; then
      echo "policy fail to take effect. in-use policy is $policyInUse, expected one is $1"
      ca_recover_and_exit 1;
   elif [[ "$policyInUse" == "$1" ]]; then
      echo "policy $policyInUse take effect."
   fi
}

#function xx_echo_log_in_color ()
#{
#   if [[ "$#" -ne 2 || -z "$1" || -z "$2" ]]; then
#      echo "error: $0: please specify content and color."
#      if [[ -n "$1" ]]; then
#         echo -e "\\033[31m${1}\\033[0m"
#      fi
#   else
#      echo -e "\\033[${2}m${1}\\033[0m"
#  fi
#}
#!/bin/sh
function fw_env_check ()
{
  #only check mandatory env
  local user_env_list="$SPARK_HOME,$HADOOP_HOME,$EGO_TOP,$SYM_MASTER_HOST,$TEST_TOOL_HOME,$JAVA_HOME,$HOST_NUM,$SLOTS_PER_HOST,$DIST_FILE_SYSTEM"
  local IFS_OLD=$IFS
  IFS=','
  for user_env in $user_env_list; do
    if [[ -z $user_env ]]; then
       echo "envrionment check failed. please make sure ./lib/environment.conf is well configured."
       exit 1;
    fi
  done
  IFS=$IFS_OLD
}
function fw_get_ego_version ()
{
  local ego_version=""
  local egosh_cmd=`which egosh`
  #echo "egosh_cmd = $egosh_cmd"
  if [[ "$egosh_cmd" == *egosh ]]; then
     local logon_status=`egosh user logon -u Admin -x Admin`
     #echo "logon_status = $logon_status"
     if [[ "$logon_status" == "Logged on successfully" ]]; then
        ego_version=`egosh -V 2>&1 | awk '/Platform EGO/ {print $(NF-5)}'`
     else
        echo "error: egosh logon failed." >&2
     fi
  else
    echo "error: egosh command cannot be found." >&2
  fi 
  echo $ego_version
}
function fw_get_spark_on_ego_version ()
{
  $SPARK_HOME/bin/spark-submit --egoversion |grep "Build"
}
function fw_get_spark_version ()
{
  local spark_version=""
  if [[ -f $SPARK_HOME/bin/spark-submit && -x $SPARK_HOME/bin/spark-submit ]]; then
     spark_version=`$SPARK_HOME/bin/spark-submit --version 2>&1 | awk '/version/ {print $NF}'`
  else
     echo "error: $SPARK_HOME/bin/spark-submit does not exist or has no executable right." >&2
  fi
  echo $spark_version  
}
function fw_get_hadoop_version ()
{
  local hadoop_version=""
  if [[ -f $HADOOP_HOME/bin/hadoop && -x $HADOOP_HOME/bin/hadoop ]]; then
    hadoop_version=`$HADOOP_HOME/bin/hadoop version | awk '/Hadoop/ {print $NF}'`
  else
    echo "error: $HADOOP_HOME/bin/hadoop does not exist or has no executable right." >&2
  fi
  echo $hadoop_version
}
function fw_report_create_html_report ()
{
if [[ -f $TEST_TOOL_HOME/conf/soe_report.html.template ]]; then
   cp $TEST_TOOL_HOME/conf/soe_report.html.template $global_report_dir/soe_report.html
   sed -i 's#{SPARK_HOME}#'$SPARK_HOME'#g' $global_report_dir/soe_report.html
   sed -i 's/{global_ego_version}/'$global_ego_version'/g' $global_report_dir/soe_report.html
   sed -i 's/{global_spark_version}/'$global_spark_version'/g' $global_report_dir/soe_report.html
   sed -i 's#{global_soe_version}#'"$global_soe_version"'#g' $global_report_dir/soe_report.html
   sed -i 's/{global_test_start_time}/'"$global_test_start_time"'/g' $global_report_dir/soe_report.html
   sed -i 's/{global_test_end_time}/'"$global_test_end_time"'/g' $global_report_dir/soe_report.html
   sed -i 's#{global_report_dir}#'$global_report_dir'#g' $global_report_dir/soe_report.html
   sed -i 's/{global_case_total}/'$global_case_total'/g' $global_report_dir/soe_report.html
   sed -i 's/{global_case_pass_rate}/'$global_case_pass_rate'/g' $global_report_dir/soe_report.html
   sed -i 's/{global_case_pass}/'$global_case_pass'/g' $global_report_dir/soe_report.html
   sed -i 's/{global_case_fail}/'$global_case_fail'/g' $global_report_dir/soe_report.html
   sed -i 's/{global_case_timeout}/'$global_case_timeout'/g' $global_report_dir/soe_report.html
   sed -i 's/{global_case_skip}/'$global_case_skip'/g' $global_report_dir/soe_report.html
   echo "please find html report at $global_report_dir/soe_report.html"
else
    echo "error: soe_report.html.template doesnot exist, cannot create report in html."
fi   
}
